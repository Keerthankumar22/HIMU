# HIMU
This project focuses on intention classification, which aims to determine whether a personâ€™s expressed intention (via speech, text, or behavior) matches their actual intention. It leverages a multi-modal deep learning model that integrates information from:

ğŸ“ Text (spoken/written content)

ğŸ”Š Audio (tone, pitch, speech patterns)

ğŸ¥ Video (facial expressions, gestures)

The developed model achieves an accuracy of 78.83%, showcasing its potential in understanding complex human behavior through multi-modal signals.

ğŸ—‚ï¸ Project Structure
All files related to this project are located in the intention_identification/ directory.

ğŸ“ Datasets
Includes both raw and preprocessed datasets required for training and testing.

ğŸ“’ Notebooks
demo_project.ipynb

Full training pipeline: data loading, preprocessing, model building, training, and evaluation.

Use this notebook if you want to train the model from scratch or fine-tune it.

demo_intention_identification.ipynb

Inference pipeline for testing the trained model on video samples.

Ideal for running predictions on new inputs and validating results.

âš™ï¸ Getting Started
You can run the notebooks using:

Jupyter Notebook (locally) â€“ Installation Guide

Google Colab (cloud-based, no setup required) â€“ Open in Colab

Ensure all dependencies (e.g., torch, transformers, opencv, etc.) are installed before execution. GPU acceleration is highly recommended for training and real-time inference.

ğŸ“ˆ Model Performance
Accuracy Achieved: 78.83%

Trained on multi-modal inputs for robust classification
